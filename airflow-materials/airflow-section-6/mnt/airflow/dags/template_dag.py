import sys
import airflow
from airflow import DAG, macros
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.operators.postgres_operator import PostgresOperator
from datetime import datetime, timedelta

# Would be cleaner to add the path to the PYTHONPATH variable
sys.path.insert(1, '/usr/local/airflow/dags/scripts')

from process_logs import process_logs_func

TEMPLATED_LOG_DIR = """{{ var.value.source_path }}/data/{{ macros.ds_format(ts_nodash, "%Y%m%dT%H%M%S", "%Y-%m-%d-%H-%M") }}/"""

default_args = {
            "owner": "Airflow",
            "start_date": airflow.utils.dates.days_ago(1),
            "depends_on_past": False,
            "email_on_failure": False,
            "email_on_retry": False,
            "email": "youremail@host.com",
            "retries": 1
        }

with DAG(dag_id="template_dag", schedule_interval="@daily", default_args=default_args) as dag:

    # t0 = BashOperator(
    #         task_id="t0",
    #         bash_command="echo {{ds}}")

    # t0 = BashOperator(
    #         task_id="t0",
    #         bash_command="echo {{var.value.CASSANDRA_LOGIN}}")

    #The macro ds_format takes an input string which is another predefined variable named ts_nodash corresponding to the execution
    # date of the DAG in iso format, and outputs another string as
    # as specified in the output format. So, in our case, the execution date given without dashes having the format
    # defined here, will be transformed to match with the output format given here.
    # Basically, we add dashes between year, month, day and so on.
    # Let’s add “ts_nodash” just before the call of the macro so that you will see the original value.
    t0 = BashOperator(
            task_id="t0",
            bash_command="echo {{ts_nodash}} - {{macros.ds_format(ts_nodash, '%Y%m%dT%H%M%S', '%Y-%m-%d-%H-%M')}}")


    # t0 = BashOperator(
    #         task_id="t0",
    #         bash_command="echo {{ macros.ds_format(ts_nodash, '%Y%m%dT%H%M%S', '%Y-%m-%d-%H-%M') }}")
    #
    t1 = BashOperator(
            task_id="generate_new_logs",
            bash_command="./scripts/generate_new_logs.sh",
            params={'filename': 'log.csv'})

    #check if log file is generated by task t1
    t2 = BashOperator(
           task_id="logs_exist",
           bash_command="test -f " + TEMPLATED_LOG_DIR + "log.csv",
           )

    #apply some changes to logs.csv and save as processed_log.csv
    t3 = PythonOperator(
           task_id="process_logs",
           python_callable=process_logs_func,
           provide_context=True,
           templates_dict={'log_dir':TEMPLATED_LOG_DIR},
           params={'filename': 'log.csv'}
           )

    t0 >> t1 >> t2 >> t3